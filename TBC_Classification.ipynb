{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8U-dwHI2Of1D"
   },
   "source": [
    "## Classification TBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDy3gy2CQAfI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torchvision.models\n",
    "from torchsummary import summary\n",
    "import timm\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B1owLuSjZnRZ"
   },
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['lr'] = [0.001 , 0.0001 , 0.00001]\n",
    "args['batch']= [16, 32, 64]\n",
    "args['optimizer'] = ['adam', 'rmsprop', 'sgdm', 'sgd']\n",
    "args['epoch'] = 20\n",
    "args['class'] = ['Bakteri TBC', 'non TBC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSHgATu_ZI52"
   },
   "outputs": [],
   "source": [
    "def save_pickle(filename, data):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "    print(f'berhasil menyimpan pkl {filename}')\n",
    "\n",
    "def write_pickle(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        loaded_cache = pickle.load(file)\n",
    "    print(f'Berhasil Load pkl {filename}')\n",
    "    return loaded_cache\n",
    "\n",
    "class efficientNetC(nn.Module):\n",
    "    def __init__(self, version):\n",
    "        super(efficientNetC, self).__init__()\n",
    "        self.efficientnet = EfficientNet.from_pretrained('efficientnet-'+version)\n",
    "        in_features = self.efficientnet._fc.in_features\n",
    "        num_class = len(args['class'])\n",
    "        self.efficientnet._fc = nn.Linear(in_features, num_class)\n",
    "        self.version = version\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.efficientnet(x)\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction_ratio),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction_ratio, in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.squeeze(x)\n",
    "        y = y.view(y.size(0), -1)\n",
    "        y = self.excitation(y).view(y.size(0), -1, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "class SEResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(SEResNetBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.se_block = SEBlock(out_channels)\n",
    "\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out = self.se_block(out)\n",
    "\n",
    "        identity = self.downsample(identity)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class SEResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SEResNet18, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(SEResNetBlock, 64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(SEResNetBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(SEResNetBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(SEResNetBlock, 512, 2, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, channels, stride))\n",
    "        self.in_channels = channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(channels, channels, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        self.model = torchvision.models.googlenet(pretrained=True)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.model = torchvision.models.mobilenet_v2(num_classes=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "class ResNet152V2(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ResNet152V2, self).__init__()\n",
    "        self.model = torchvision.models.resnet152(weights='IMAGENET1K_V2')\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "class Classification:\n",
    "    def __init__(self, str_model):\n",
    "        self.modelname = str_model\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.epochs = args['epoch']\n",
    "        self.best_model = None\n",
    "\n",
    "    def create_model(self, model_name):\n",
    "        if model_name == 'efficientNet':\n",
    "          return  efficientNetC(\"b0\")\n",
    "        elif model_name == 'se':\n",
    "          return SEResNet18()\n",
    "        elif model_name == 'googlenet':\n",
    "          return GoogLeNet()\n",
    "        elif model_name == 'mobilenetv2':\n",
    "          return MobileNetV2()\n",
    "        elif model_name == 'resnet152v2':\n",
    "          return ResNet152V2()\n",
    "\n",
    "    def compile(self, model, optim, lr):\n",
    "        if optim == \"rmsprop\":\n",
    "          self.model_optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "        elif optim == \"adam\":\n",
    "          self.model_optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        elif optim == \"sgdm\":\n",
    "          self.model_optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.001)\n",
    "        elif optim == \"sgd\":\n",
    "          self.model_optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    def preprocessing(self, x, y, xval, yval, batch):\n",
    "        if self.Train == True:\n",
    "          self.data_x_train, self.data_y_train = x.permute(0,3,1,2)/255.0, y\n",
    "          self.data_x_val, self.data_y_val = xval.permute(0,3,1,2)/255.0, yval\n",
    "\n",
    "          self.train_loader = DataLoader(TensorDataset(self.data_x_train, self.data_y_train), batch_size = batch, shuffle=True)\n",
    "          self.val_loader = DataLoader(TensorDataset(self.data_x_val, self.data_y_val), batch_size = batch, shuffle=True)\n",
    "\n",
    "        else:\n",
    "          self.data_x_test, self.data_y_test = x.permute(0,3,1,2)/255.0, y\n",
    "          self.test_loader = DataLoader(TensorDataset(self.data_x_test, self.data_y_test), batch_size = batch, shuffle=True)\n",
    "\n",
    "    def save_model(self, model, filename):\n",
    "        torch.save(model.state_dict(), args['model']+filename+\".pth\")\n",
    "\n",
    "    def load_model(self, model, filename, device):\n",
    "        return torch.load(args['model']+filename+\".pth\", map_location=device)\n",
    "\n",
    "    def train(self, model, optimizer, criterion, train_loader):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        alpha, beta = 1e-5, 1e-3\n",
    "        for id_batch, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predict = torch.max(outputs.data, 1)\n",
    "            correct += (predict == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        return model, train_loss, train_acc\n",
    "\n",
    "    def valid(self, model, criterion, valid_loader):\n",
    "        model.eval()\n",
    "        running_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_prediction, all_target = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for batch_idx, (images, labels) in enumerate(valid_loader):\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                outputs = model(images)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                _, predict = torch.max(outputs.data, 1)\n",
    "                correct += (predict == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "\n",
    "                all_prediction.extend(predict.cpu().numpy())\n",
    "                all_target.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss = running_loss / len(valid_loader)\n",
    "        val_acc = correct / total\n",
    "        eval=self.evaluate(all_prediction, all_target)\n",
    "\n",
    "        return model, val_loss, val_acc, eval\n",
    "\n",
    "    def fit(self, X, y, xval, yval, device):\n",
    "        self.Train = True\n",
    "        self.device=device\n",
    "\n",
    "        best_acc_tunning = 0\n",
    "        ####tunning\n",
    "        #lr, batch, optimizer\n",
    "        for lr_use in args['lr']:\n",
    "            for batch_use in args['batch']:\n",
    "                for optimizer_use in args['optimizer']:\n",
    "\n",
    "                    model_use = self.create_model(self.modelname)\n",
    "                    model_use = model_use.to(self.device)\n",
    "                    self.criterion = self.criterion.to(device)\n",
    "                    model_stats = {}\n",
    "                    loss_train, acc_train, loss_val, acc_val, perform=[], [], [], [], []\n",
    "                    best_loss = 999\n",
    "                    time_start = time.time()\n",
    "\n",
    "                    proses_name = f\"model_{self.modelname}_lr_{lr_use}_batch_{batch_use}_optimizer_{optimizer_use}\"\n",
    "                    print(f\"Tunning {proses_name}\")\n",
    "\n",
    "                    self.compile(model_use, optimizer_use, lr_use)\n",
    "                    self.preprocessing(X,y,xval,yval, batch_use)\n",
    "\n",
    "                    for epoch in range(self.epochs):\n",
    "\n",
    "                      model_use, train_loss, train_acc = self.train(model_use, self.model_optimizer, self.criterion, self.train_loader)\n",
    "                      model_use, val_loss, val_acc, eval = self.valid(model_use, self.criterion, self.val_loader)\n",
    "\n",
    "                      loss_train.append(train_loss)\n",
    "                      acc_train.append(train_acc)\n",
    "                      loss_val.append(val_loss)\n",
    "                      acc_val.append(val_acc)\n",
    "                      perform.append(eval)\n",
    "\n",
    "                      #simpan model\n",
    "                      print(f\"Epoch : {epoch+1} train loss : {train_loss} train acc : {train_acc} val loss : {val_loss} val acc : {val_acc} \")\n",
    "                      if train_loss < best_loss :\n",
    "                          print(\"Menyimpan Model\")\n",
    "                          self.save_model(model_use, proses_name)\n",
    "                          best_loss = train_loss\n",
    "                    time_stop = time.time()\n",
    "\n",
    "                    #simpan semua loss fold\n",
    "                    model_stats['loss_training'], model_stats['acc_training']=loss_train, acc_train\n",
    "                    model_stats['loss_validasi'], model_stats['acc_validasi']=loss_val, acc_val\n",
    "\n",
    "                    df = pd.DataFrame(model_stats)\n",
    "                    df.to_excel(args['performa_excel']+proses_name+\".xlsx\", index=False)\n",
    "\n",
    "                    model_stats['timer']=(time_stop-time_start)\n",
    "                    model_stats['perform']=perform\n",
    "                    save_pickle(args['performa']+f\"perform_{proses_name}.pkl\", model_stats)\n",
    "                    print(f\"with time : {model_stats['timer']} loss : {loss_val[-1]} acc : {acc_val[-1]} precision : {model_stats['perform'][-1][1]} recall : {model_stats['perform'][-1][2]} f1 : {model_stats['perform'][-1][3]}\")\n",
    "                    if acc_val[-1] > best_acc_tunning:\n",
    "                        best_acc_tunning = acc_val[-1]\n",
    "                        self.best_model = proses_name\n",
    "                    model_use=None\n",
    "        print(f\"FINAL BEST MODEL WITH ACC : {best_acc_tunning}\")\n",
    "        print(self.best_model)\n",
    "\n",
    "    def best_model_graph(self):\n",
    "      best_path = args['performa']+\"perform_\"+self.best_model+\".pkl\"\n",
    "      best_perform = write_pickle(best_path)\n",
    "      acc_train, acc_val, loss_train, loss_val, timer = best_perform['acc_training'], best_perform['acc_validasi'], best_perform['loss_training'], best_perform['loss_validasi'], best_perform['timer']\n",
    "\n",
    "      print(f\"Training Time for {timer}\")\n",
    "      plt.figure(figsize=(12, 5))\n",
    "      plt.plot(loss_train, label='Training Loss')\n",
    "      plt.plot(loss_val, label='Validasi Loss')\n",
    "      plt.xlabel('Epoch')\n",
    "      plt.ylabel('Loss')\n",
    "      plt.title(f'loss {self.best_model}')\n",
    "      plt.legend()\n",
    "      plt.show()\n",
    "\n",
    "      plt.figure(figsize=(12, 5))\n",
    "      plt.plot(acc_train, label='Training Accuracy')\n",
    "      plt.plot(acc_val, label='Validation Accuracu')\n",
    "      plt.xlabel('Epoch')\n",
    "      plt.ylabel('Accuracy')\n",
    "      plt.title(f'Accuracy Of {self.best_model}')\n",
    "      plt.legend()\n",
    "      plt.show()\n",
    "\n",
    "    def predict(self,  X,y, device):\n",
    "      self.Train = False\n",
    "      self.device = device\n",
    "      model = self.create_model(self.modelname)\n",
    "      # Split the string by underscores\n",
    "      tokens = self.best_model.split('_')\n",
    "      batch_index = tokens.index('batch')\n",
    "      batch_size = int(tokens[batch_index + 1])\n",
    "\n",
    "      model.load_state_dict(self.load_model(model, self.best_model, self.device) ,strict=False)\n",
    "      self.preprocessing(X,y, None, None, batch_size)\n",
    "\n",
    "      # testing phase\n",
    "      _, test_loss, test_acc, perform = self.valid(model, self.criterion, self.test_loader)\n",
    "\n",
    "      print(f\"Hasil Testing\")\n",
    "\n",
    "      plt.figure(figsize=(6, 6))\n",
    "      sn.heatmap(perform[4], annot=True, fmt='d', cmap='Blues', xticklabels=args['class'], yticklabels=args['class'])\n",
    "      plt.xlabel('Predicted')\n",
    "      plt.ylabel('Target')\n",
    "      plt.title('Confusion Matrix')\n",
    "      plt.show()\n",
    "\n",
    "      print(f\"Loss : {test_loss}\")\n",
    "      print(f\"Accuracy : {test_acc}\")\n",
    "      print(f\"Precision : {perform[1]}\")\n",
    "      print(f\"Recall : {perform[2]}\")\n",
    "      print(f\"F1-score : {perform[3]}\")\n",
    "\n",
    "    def evaluate(self, predict, target):\n",
    "      conf_matrix = torch.from_numpy(confusion_matrix(target, predict))\n",
    "      metrics_per_class = {}\n",
    "      for i in range(conf_matrix.size(0)):\n",
    "        TP = conf_matrix[i, i]\n",
    "        FP = torch.sum(conf_matrix[:, i]) - TP\n",
    "        FN = torch.sum(conf_matrix[i, :]) - TP\n",
    "        TN = torch.sum(conf_matrix) - TP -FP -FN\n",
    "\n",
    "        accuracy = (TP + TN) / torch.sum(conf_matrix)\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        metrics_per_class[args['class'][i]] = {\n",
    "            'acc': accuracy.item(),\n",
    "            'precision': precision.item(),\n",
    "            'recall': recall.item(),\n",
    "            'f1': f1.item()}\n",
    "      accuracy_total = sum(metric['acc'] for metric in metrics_per_class.values()) / conf_matrix.size(0)\n",
    "      precision_total = sum(metric['precision'] for metric in metrics_per_class.values()) / conf_matrix.size(0)\n",
    "      recall_total = sum(metric['recall'] for metric in metrics_per_class.values()) / conf_matrix.size(0)\n",
    "      f1_total = sum(metric['f1'] for metric in metrics_per_class.values()) / conf_matrix.size(0)\n",
    "      return [accuracy_total, precision_total, recall_total, f1_total, conf_matrix, metrics_per_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5t72uddcO0Gx",
    "outputId": "3f2217d2-e4c1-459c-8354-aa00481c2a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain : torch.Size([1012, 224, 224, 3])\n",
      "xtest : torch.Size([254, 224, 224, 3])\n",
      "ytrain : torch.Size([1012])\n",
      "ytest : torch.Size([254])\n"
     ]
    }
   ],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(dataset['images'], dataset['class'], test_size=0.2, random_state=42, stratify=dataset['class'])\n",
    "print(f\"xtrain : {xtrain.shape}\")\n",
    "print(f\"xtest : {xtest.shape}\")\n",
    "print(f\"ytrain : {ytrain.shape}\")\n",
    "print(f\"ytest : {ytest.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81RJh4jZDEtq",
    "outputId": "cf4b1621-c7f9-4eb0-f05a-d2f0fbcf0258"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain_val : torch.Size([809, 224, 224, 3])\n",
      "xval : torch.Size([203, 224, 224, 3])\n",
      "ytrain_val : torch.Size([809])\n",
      "yval : torch.Size([203])\n"
     ]
    }
   ],
   "source": [
    "xtrain_val, xval, ytrain_val, yval = train_test_split(xtrain, ytrain, test_size=0.2, random_state=42, stratify=ytrain)\n",
    "print(f\"xtrain_val : {xtrain_val.shape}\")\n",
    "print(f\"xval : {xval.shape}\")\n",
    "print(f\"ytrain_val : {ytrain_val.shape}\")\n",
    "print(f\"yval : {yval.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1_GD_RJO0dL"
   },
   "source": [
    "## Training CNN\n",
    "- EfficientNet\n",
    "- Squeeze and Excitation\n",
    "- GoogleNet\n",
    "- ResNetV2\n",
    "- ResNet152V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqi3eTeqa47a"
   },
   "source": [
    "## A. EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3YXu256FK14",
    "outputId": "8152fac0-9c41-4f87-c4f3-e94ea55d964e"
   },
   "outputs": [],
   "source": [
    "# model = efficientNetC(\"b0\")\n",
    "classifier = Classification(\"efficientNet\")\n",
    "classifier.fit(xtrain_val, ytrain_val, xval, yval, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 992
    },
    "id": "kkNG7Z_HF6bw",
    "outputId": "f871236e-fcf8-4a23-969c-5e623ec9a46e"
   },
   "outputs": [],
   "source": [
    "classifier.best_model_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "id": "N53B01FEK4Ro",
    "outputId": "476789d3-7faf-4332-a9e5-911b391d3d14"
   },
   "outputs": [],
   "source": [
    "classifier = Classification(\"efficientNet\")\n",
    "classifier.best_model = 'model_efficientNet_lr_0.001_batch_32_optimizer_adam'\n",
    "classifier.predict(xtest, ytest, 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4akQUXmO-VB"
   },
   "source": [
    "## B. SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXEV3pxwC2e_",
    "outputId": "545dc80a-8d04-46aa-83b9-1cff9ffe2001"
   },
   "outputs": [],
   "source": [
    "# model = efficientNetC(\"b0\")\n",
    "classifier = Classification(\"se\")\n",
    "classifier.fit(xtrain_val, ytrain_val, xval, yval, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 992
    },
    "id": "NBG9vxlPYreZ",
    "outputId": "0ec9f0a4-12bc-40a4-bbbe-8949c4412ded"
   },
   "outputs": [],
   "source": [
    "classifier.best_model_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "id": "0iI7CgCCYu4Z",
    "outputId": "edf08ab0-e22d-44af-8bae-4f5f26a411f3"
   },
   "outputs": [],
   "source": [
    "classifier.predict(xtest, ytest, 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dFru2x0Y4SX"
   },
   "source": [
    "## C. GoogleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imPIVS18Y7tw",
    "outputId": "df06f7d8-3360-4e15-c2b7-811a40afb7af"
   },
   "outputs": [],
   "source": [
    "classifier = Classification(\"googlenet\")\n",
    "classifier.fit(xtrain_val, ytrain_val, xval, yval, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 919
    },
    "id": "_9YVm2lYW-Je",
    "outputId": "aae3958a-e831-4fcf-fb99-cb6f6a8584de"
   },
   "outputs": [],
   "source": [
    "classifier.best_model_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 758
    },
    "id": "dMkWABlUXA2d",
    "outputId": "c26c51a4-6e79-460a-ae6e-f522e60f75cc",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier.predict(xtest, ytest, 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaI_OlRKZBIG"
   },
   "source": [
    "## D. ResNet 152 V2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fr6vqvvRY6LO",
    "outputId": "6da2e04f-70ae-4946-974b-aa06ff41301a"
   },
   "outputs": [],
   "source": [
    "classifier = Classification(\"resnet152v2\")\n",
    "classifier.fit(xtrain_val, ytrain_val, xval, yval, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 922
    },
    "id": "xzxnMkU3Vh_n",
    "outputId": "c7bfaf65-f328-47d3-b751-01a0d12a14ce"
   },
   "outputs": [],
   "source": [
    "classifier.best_model_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "id": "GIy54ceYVkLI",
    "outputId": "31001792-a58d-4e8b-e47d-84433e887313"
   },
   "outputs": [],
   "source": [
    "classifier.predict(xtest, ytest, 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLOwNGOuWMBq"
   },
   "source": [
    "## E. MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YByf3WLWWRif",
    "outputId": "583c9dba-c722-4c32-8301-727fece6d7b7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier = Classification(\"mobilenetv2\")\n",
    "classifier.fit(xtrain_val, ytrain_val, xval, yval, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 992
    },
    "id": "NbsygBBeW_cy",
    "outputId": "2cc8c60a-41d9-4940-ee53-096a77c033a0"
   },
   "outputs": [],
   "source": [
    "classifier.best_model_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "id": "whQfj9FuXBrB",
    "outputId": "2beba052-9e62-425b-82bf-7329ad5fef38"
   },
   "outputs": [],
   "source": [
    "classifier.predict(xtest, ytest, 'cpu')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
